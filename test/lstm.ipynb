{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_ROOT_PATH = \"/home/XuHaoshuai/Project/HumanIE-IPM-experiment-2.0/lstm\"\n",
    "relation = 'couple'\n",
    "\n",
    "\n",
    "training_set = pd.read_csv(os.path.join(DATA_ROOT_PATH, 'training_set', relation + '.csv'))\n",
    "test_set = pd.read_csv(os.path.join(DATA_ROOT_PATH, 'test_set', relation + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 20826)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "def get_vocab(sent_list):\n",
    "    tokenized_data = [[word for word in sent.split(' ') if word != ''] for sent in sent_list]\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return vocab(counter, min_freq=1)\n",
    "\n",
    "train_vocab = get_vocab(training_set['processed_sent'].tolist())\n",
    "test_vocab = get_vocab(test_set['processed_sent'].tolist())\n",
    "'# words in vocab:', len(test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent_list, label_list, vocab):\n",
    "    max_l = 50  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = [[word for word in sent.split(' ') if word != ''] for sent in sent_list]\n",
    "    features = torch.tensor([pad([vocab.get_stoi()[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor(label_list)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 50\n",
    "train_set = Data.TensorDataset(*preprocess(training_set['processed_sent'].tolist(), training_set['human'].tolist(), train_vocab))\n",
    "test_set = Data.TensorDataset(*preprocess(test_set['processed_sent'].tolist(), test_set['label'].tolist() ,test_vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([50, 200]) y torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        # embedding\n",
    "        wvmodel = Word2Vec.load(\"/home/XuHaoshuai/Project/HumanIE-IPM-experiment-2.0/word2vec/word2vec.model\")\n",
    "        vocab_size = len(wvmodel.wv)\n",
    "        vector_size = wvmodel.vector_size\n",
    "        weight = torch.randn(vocab_size, vector_size)\n",
    "        words = wvmodel.wv.index_to_key\n",
    "        word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "        idx_to_word = {i: word for i, word in enumerate(words)}\n",
    "        for i in range(len(wvmodel.wv.index_to_key)):\n",
    "            try:\n",
    "                index = word_to_idx[wvmodel.wv.index_to_key[i]]\n",
    "            except:\n",
    "                continue\n",
    "        vector=wvmodel.wv.get_vector(idx_to_word[word_to_idx[wvmodel.wv.index_to_key[i]]])\n",
    "        weight[index, :] = torch.from_numpy(vector)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        \n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(train_vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.7194, train acc 0.534, test acc 0.500, time 5.7 sec\n",
      "epoch 2, loss 0.3004, train acc 0.675, test acc 0.443, time 5.4 sec\n",
      "epoch 3, loss 0.1041, train acc 0.878, test acc 0.479, time 5.4 sec\n",
      "epoch 4, loss 0.0308, train acc 0.955, test acc 0.475, time 5.4 sec\n",
      "epoch 5, loss 0.0106, train acc 0.981, test acc 0.482, time 5.4 sec\n",
      "epoch 6, loss 0.0062, train acc 0.992, test acc 0.487, time 5.4 sec\n",
      "epoch 7, loss 0.0015, train acc 0.998, test acc 0.493, time 5.4 sec\n",
      "epoch 8, loss 0.0018, train acc 0.996, test acc 0.491, time 5.3 sec\n",
      "epoch 9, loss 0.0016, train acc 0.996, test acc 0.528, time 5.3 sec\n",
      "epoch 10, loss 0.0011, train acc 0.998, test acc 0.521, time 5.4 sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "# Train the model\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "lr, num_epochs = 0.01, 10\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, os.path.join(DATA_ROOT_PATH, 'saved_model', relation + '.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiRNN(\n",
       "  (embedding): Embedding(55923, 100)\n",
       "  (encoder): LSTM(100, 100, num_layers=2, bidirectional=True)\n",
       "  (decoder): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"/home/XuHaoshuai/Project/HumanIE-IPM-experiment-2.0/lstm/saved_model/couple.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(net, vocab, sentence):\n",
    "#     \"\"\"sentence是词语的列表\"\"\"\n",
    "#     device = list(net.parameters())[0].device\n",
    "#     sentence = torch.tensor([vocab.get_stoi()[word] for word in sentence], device=device)\n",
    "#     label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "#     return 1 if label.item() == 1 else -1\n",
    "\n",
    "# predict_sentiment(net, train_vocab, ['词语','解释','指','人物二','从弟','人物一'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "062d8928aa6c74abd7a71009f94b804d3e6473753e91c42ed893a6f7b0e52b17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('humanie_ipm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
